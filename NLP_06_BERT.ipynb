{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_06_BERT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP5dyhjw2OHIcgF2hhAhxEt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alxiom/Basic-NLP/blob/main/NLP_06_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7Yb6PeUfgW9",
        "outputId": "d0657f39-4413-4251-dd87-c11896759792"
      },
      "source": [
        "!pip install tokenizers\n",
        "!git clone https://github.com/alxiom/Basic-NLP.git\n",
        "!wget https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv -O Basic-NLP/data/chat.csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.3)\n",
            "fatal: destination path 'Basic-NLP' already exists and is not an empty directory.\n",
            "--2021-09-06 15:00:09--  https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 889842 (869K) [text/plain]\n",
            "Saving to: ‘Basic-NLP/data/chat.csv’\n",
            "\n",
            "Basic-NLP/data/chat 100%[===================>] 868.99K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-09-06 15:00:09 (46.1 MB/s) - ‘Basic-NLP/data/chat.csv’ saved [889842/889842]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg3WsSNeegvl"
      },
      "source": [
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as ftn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tokenizers import CharBPETokenizer\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBtByHlHepNq"
      },
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = \"cpu\"\n",
        "special = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"<sep>\", \"<cls>\", \"<mask>\"]\n",
        "\n",
        "train_bert = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsvTUk-zfcy8"
      },
      "source": [
        "tokenizer = CharBPETokenizer(vocab=\"Basic-NLP/data/vocab.json\", merges=\"Basic-NLP/data/merges.txt\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcjfS_Fle1s4"
      },
      "source": [
        "@dataclass\n",
        "class BertConfig:\n",
        "    seq_len: int = 16\n",
        "    vocab_size: int = 8000\n",
        "    num_encoder_layers: int = 6\n",
        "    embedding_dim: int = 512\n",
        "    num_heads: int = 8\n",
        "    hidden_dim: int = 2048\n",
        "    dropout: float = 0.1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FlbZ2H9e2Mp"
      },
      "source": [
        "@dataclass\n",
        "class TrainConfig:\n",
        "    epochs: int = 10\n",
        "    batch_size: int = 64\n",
        "    learning_rate: float = 1e-4\n",
        "    weight_decay: float = 0.01\n",
        "    warmup_steps: int = 10000\n",
        "    checkpoint_path: str = None\n",
        "    num_workers: int = 0  # for DataLoader"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-drjnx-Me3um"
      },
      "source": [
        "class BertDataset(Dataset):\n",
        "\n",
        "    def __init__(self, corpus, tokenizer, seq_len):\n",
        "        super(BertDataset, self).__init__()\n",
        "        self.corpus = corpus\n",
        "        self.corpus_size = len(corpus)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.corpus_size\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        q, a, is_next_label = self.nsp_data(item)\n",
        "        q_random, q_label = self.mlm_data(q)\n",
        "        a_random, a_label = self.mlm_data(a)\n",
        "\n",
        "        q = [special.index(\"<cls>\")] + q_random + [special.index(\"<sep>\")]\n",
        "        a = a_random + [special.index(\"<eos>\")]\n",
        "\n",
        "        q_label = [special.index(\"<pad>\")] + q_label + [special.index(\"<pad>\")]\n",
        "        a_label = a_label + [special.index(\"<pad>\")]\n",
        "\n",
        "        bert_input = (q + a)[:self.seq_len]\n",
        "        bert_label = (q_label + a_label)[:self.seq_len]\n",
        "        segment_label = ([1 for _ in range(len(q))] + [2 for _ in range(len(a))])[:self.seq_len]\n",
        "\n",
        "        padding = [special.index(\"<pad>\") for _ in range(self.seq_len - len(bert_input))]\n",
        "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
        "\n",
        "        output = {\n",
        "            \"bert_input\": bert_input,\n",
        "            \"bert_label\": bert_label,\n",
        "            \"segment_label\": segment_label,\n",
        "            \"is_next\": is_next_label,\n",
        "        }\n",
        "        return {key: torch.tensor(value) for key, value in output.items()}\n",
        "\n",
        "    def mlm_data(self, text):\n",
        "        tokens = self.tokenizer.encode(text).ids\n",
        "        labels = []\n",
        "        for i in range(len(tokens)):\n",
        "            masking_prob = random.random()\n",
        "            if masking_prob < 0.15:  # 전체의 15% masking\n",
        "                labels.append(tokens[i])\n",
        "                mask_type_prob = random.random()\n",
        "                if mask_type_prob < 0.8:  # 마스킹 대상 중 80% 마스킹\n",
        "                    tokens[i] = special.index(\"<mask>\")\n",
        "                elif mask_type_prob < 0.9:  # 마스킹 대상 중 10% random replace\n",
        "                    tokens[i] = random.randrange(self.tokenizer.get_vocab_size())\n",
        "            else:\n",
        "                labels.append(0)\n",
        "        return tokens, labels\n",
        "\n",
        "    def nsp_data(self, item):\n",
        "        q, a = self.get_corpus_line(item)\n",
        "\n",
        "        # query, answer, label(isNext: 1, isNotNext: 0)\n",
        "        if random.random() > 0.5:\n",
        "            return q, a, 1\n",
        "        else:\n",
        "            return q, self.get_random_answer_line(), 0\n",
        "\n",
        "    def get_corpus_line(self, item):\n",
        "        return self.corpus[item][0], self.corpus[item][1]\n",
        "\n",
        "    def get_random_answer_line(self):\n",
        "        return self.corpus[random.randrange(self.corpus_size)][1]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIx4YMlAgCay"
      },
      "source": [
        "def positional_encoding(seq_len: int, embedding_dim: int) -> Tensor:\n",
        "    pe = np.zeros([seq_len, embedding_dim])\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(0, embedding_dim, 2):\n",
        "            pe[pos, i] = np.sin(pos / (1e+4 ** ((2 * i) / embedding_dim)))\n",
        "            pe[pos, i + 1] = np.cos(pos / (1e+4 ** ((2 * (i + 1)) / embedding_dim)))\n",
        "    return torch.from_numpy(pe).float()\n",
        "\n",
        "\n",
        "def mask(x: Tensor, mask_value: float = 0.0, mask_diagonal: bool = False) -> Tensor:\n",
        "    seq_len = x.size(1)\n",
        "    indices = torch.triu_indices(seq_len, seq_len, offset=0 if mask_diagonal else 1)\n",
        "    x[:, indices[0], indices[1]] = mask_value\n",
        "    return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLYrY6nngEQF"
      },
      "source": [
        "def scaled_dot_product_attention(pad_mask: Tensor, query: Tensor, key: Tensor, value: Tensor, masking: bool) -> Tensor:\n",
        "    dot_prod = query.bmm(key.transpose(1, 2))\n",
        "    if masking:\n",
        "        dot_prod = mask(dot_prod, float(\"-inf\"))\n",
        "    scale = query.size(-1) ** 0.5\n",
        "    pad_mask = pad_mask.unsqueeze(1).repeat(1, pad_mask.size(1), 1)\n",
        "    scaled_dot_product = (dot_prod / scale).masked_fill_(pad_mask, -1e+9)\n",
        "    attention = ftn.softmax(scaled_dot_product, dim=-1).bmm(value)\n",
        "    return attention"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv7GBylAgFzL"
      },
      "source": [
        "class AttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, query_dim: int, value_dim: int, masking: bool):\n",
        "        super(AttentionHead, self).__init__()\n",
        "        self.q = nn.Linear(embedding_dim, query_dim)\n",
        "        self.k = nn.Linear(embedding_dim, query_dim)  # key_dim = query_dim\n",
        "        self.v = nn.Linear(embedding_dim, value_dim)\n",
        "        self.masking = masking\n",
        "\n",
        "    def forward(self, pad_mask: Tensor, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "        return scaled_dot_product_attention(pad_mask, self.q(query), self.k(key), self.v(value), self.masking)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads: int, embedding_dim: int, query_dim: int, value_dim: int, masking: bool = False):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [AttentionHead(embedding_dim, query_dim, value_dim, masking) for _ in range(num_heads)]\n",
        "        )\n",
        "        self.linear = nn.Linear(num_heads * value_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, pad_mask: Tensor, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "        concat_heads = torch.cat([head(pad_mask, query, key, value) for head in self.heads], dim=-1)\n",
        "        return self.linear(concat_heads)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR2p37fzgHqg"
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim: int = 512, hidden_dim: int = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x) -> Tensor:\n",
        "        return self.ff(x)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdNQzgCpgJdM"
      },
      "source": [
        "class Residual(nn.Module):\n",
        "\n",
        "    def __init__(self, sublayer: nn.Module, input_dim: int = 512, dropout: float = 0.1):\n",
        "        super(Residual, self).__init__()\n",
        "        self.sublayer = sublayer\n",
        "        self.norm = nn.LayerNorm(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, *x: Tensor) -> Tensor:\n",
        "        skip = 0 if len(x) == 1 else 1\n",
        "        return self.norm(x[skip] + self.dropout(self.sublayer(*x)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Dp5rMngLIW"
      },
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dim: int = 512,\n",
        "            num_heads: int = 6,\n",
        "            hidden_dim: int = 2048,\n",
        "            dropout: float = 0.1,\n",
        "    ):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        query_dim = value_dim = embedding_dim // num_heads\n",
        "        self.self_attention = Residual(\n",
        "            MultiHeadAttention(num_heads, embedding_dim, query_dim, value_dim),\n",
        "            input_dim=embedding_dim,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.feed_forward = Residual(\n",
        "            FeedForward(embedding_dim, hidden_dim),\n",
        "            input_dim=embedding_dim,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor, pad_mask: Tensor) -> Tensor:\n",
        "        x = self.self_attention(pad_mask, x, x, x)\n",
        "        x = self.feed_forward(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_layers: int = 6,\n",
        "            embedding_dim: int = 512,\n",
        "            num_heads: int = 8,\n",
        "            hidden_dim: int = 2048,\n",
        "            dropout: float = 0.1,\n",
        "    ):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = nn.ModuleList(\n",
        "            [TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor, pad_mask: Tensor) -> Tensor:\n",
        "        seq_len = x.size(1)\n",
        "        embedding_dim = x.size(2)\n",
        "        x += positional_encoding(seq_len, embedding_dim)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, pad_mask)\n",
        "        return x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqgXSBCfgP_Y"
      },
      "source": [
        "class Bert(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(Bert, self).__init__()\n",
        "        self.embedding_dim = config.embedding_dim\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.embedding_dim, padding_idx=0)\n",
        "        self.segment_embedding = nn.Embedding(3, config.embedding_dim, padding_idx=0)\n",
        "        self.encoder = TransformerEncoder(\n",
        "            num_layers=config.num_encoder_layers,\n",
        "            embedding_dim=config.embedding_dim,\n",
        "            num_heads=config.num_heads,\n",
        "            hidden_dim=config.hidden_dim,\n",
        "            dropout=config.dropout,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor, segment_label: Tensor):\n",
        "        pad_mask = x == 0\n",
        "        x_token_embedding = self.token_embedding(x)\n",
        "        seq_len = x_token_embedding.size(1)\n",
        "        embedding_dim = x_token_embedding.size(2)\n",
        "        x_segment_embedding = self.segment_embedding(segment_label)\n",
        "        x = x_token_embedding + x_segment_embedding + positional_encoding(seq_len, embedding_dim)\n",
        "        x = self.encoder(x, pad_mask)\n",
        "        return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdfUzVlcmDYs"
      },
      "source": [
        "class MaskedLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, vocab_size: int):\n",
        "        super(MaskedLanguageModel, self).__init__()\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):  \n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "class NextSentencePrediction(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim: int):\n",
        "        super(NextSentencePrediction, self).__init__()\n",
        "        self.linear = nn.Linear(embedding_dim, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x[:, 0])\n",
        "\n",
        "\n",
        "class BertLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_model: Bert):\n",
        "        super(BertLanguageModel, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.mlm = MaskedLanguageModel(self.bert_model.embedding_dim, self.bert_model.vocab_size)\n",
        "        self.nsp = NextSentencePrediction(self.bert_model.embedding_dim)\n",
        "\n",
        "    def forward(self, x, segment_label):\n",
        "        x = self.bert_model(x, segment_label)\n",
        "        return self.mlm(x), self.nsp(x)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvEKKv08gUFN"
      },
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, bert_model, train_data, config):\n",
        "        super(Trainer, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.model = BertLanguageModel(self.bert_model)\n",
        "        self.train_data = train_data\n",
        "        self.config = config\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=config.learning_rate,\n",
        "            weight_decay=config.weight_decay,\n",
        "        )\n",
        "        self.optimizer_schedule = ScheduleOptimizer(\n",
        "            self.optimizer,\n",
        "            self.bert_model.embedding_dim,\n",
        "            warmup_steps=config.warmup_steps,\n",
        "        )\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=special.index(\"<pad>\"))\n",
        "        self.start_epoch = 1\n",
        "        self.epochs = config.epochs\n",
        "\n",
        "    def run(self):\n",
        "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
        "            train_data_loader = DataLoader(\n",
        "                self.train_data,\n",
        "                batch_size=self.config.batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=self.config.num_workers,\n",
        "            )\n",
        "\n",
        "            print(f\"run {epoch} epoch...\")\n",
        "            self.model.train()\n",
        "            mlm_loss, nsp_loss = self.run_epoch(train_data_loader)\n",
        "            total_loss = mlm_loss + nsp_loss\n",
        "            print(f\"Epoch: {epoch:2d} / MLM: {mlm_loss:.4f} / NSP: {nsp_loss:.4f} / total loss: {total_loss:.4f}\")\n",
        "\n",
        "        self.save_checkpoint()\n",
        "\n",
        "    def run_epoch(self, data_loader):\n",
        "        epoch_mlm_loss = 0.0\n",
        "        epoch_nsp_loss = 0.0\n",
        "        epoch_count = 0\n",
        "\n",
        "        for data in data_loader:\n",
        "            batch_size = len(data)\n",
        "            mlm_output, nsp_output = self.model(data[\"bert_input\"], data[\"segment_label\"])\n",
        "            mlm_loss = self.criterion(mlm_output.transpose(1, 2), data[\"bert_label\"])\n",
        "            nsp_loss = self.criterion(nsp_output, data[\"is_next\"])\n",
        "            loss = mlm_loss + nsp_loss\n",
        "            self.optimizer_schedule.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer_schedule.step_and_update_lr()\n",
        "\n",
        "            epoch_mlm_loss = (epoch_mlm_loss * epoch_count + mlm_loss.item() * batch_size) / (epoch_count + batch_size)\n",
        "            epoch_nsp_loss = (epoch_nsp_loss * epoch_count + nsp_loss.item() * batch_size) / (epoch_count + batch_size)\n",
        "            epoch_count += batch_size\n",
        "        return epoch_mlm_loss, epoch_nsp_loss\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        if self.config.checkpoint_path is not None:\n",
        "            print(\"save checkpoint...\")\n",
        "            torch.save(self.model.state_dict(), f\"{self.config.checkpoint_path}/bert.pt\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY1tXI2FgW43"
      },
      "source": [
        "class ScheduleOptimizer:\n",
        "\n",
        "    def __init__(self, optimizer, embedding_dim, warmup_steps):\n",
        "        super(ScheduleOptimizer, self).__init__()\n",
        "        self.optimizer = optimizer\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.current_steps = 0\n",
        "        self.init_lr = np.power(embedding_dim, -0.5)\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        self.update_learning_rate()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def get_lr_scale(self):\n",
        "        return np.min([\n",
        "            np.power(self.current_steps, -0.5),\n",
        "            np.power(self.warmup_steps, -1.5) * self.current_steps,\n",
        "        ])\n",
        "\n",
        "    def update_learning_rate(self):\n",
        "        self.current_steps += 1\n",
        "        lr = self.init_lr * self.get_lr_scale()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDnZwwbZgY67",
        "outputId": "6c8340a6-65f4-4a9f-cb88-20298cc64e02"
      },
      "source": [
        "# prepare dataset\n",
        "sequence_length = 16\n",
        "chat_corpus = pd.read_csv(\"Basic-NLP/data/chat.csv\", header=0).sample(1000).to_numpy()\n",
        "train_dataset = BertDataset(chat_corpus, tokenizer, sequence_length)\n",
        "\n",
        "# train data sample\n",
        "print(train_dataset[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bert_input': tensor([  5,   6, 900,   4, 991, 796, 646,   6,   6,   3,   0,   0,   0,   0,\n",
            "          0,   0]), 'bert_label': tensor([   0, 1415,    0,    0,    0,    0,    0,  671,  615,    0,    0,    0,\n",
            "           0,    0,    0,    0]), 'segment_label': tensor([1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0]), 'is_next': tensor(1)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIghvIrrgdbe"
      },
      "source": [
        "# model config\n",
        "model_config = BertConfig(\n",
        "    seq_len=sequence_length,\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    num_encoder_layers=4,\n",
        "    embedding_dim=128,\n",
        "    num_heads=4,\n",
        "    hidden_dim=512,\n",
        ")\n",
        "\n",
        "# init model\n",
        "bert = Bert(model_config)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ4V8QXWge00",
        "outputId": "c003f57e-039f-4a41-b474-8fa653f43f93"
      },
      "source": [
        "# train config\n",
        "train_config = TrainConfig(\n",
        "    epochs=10,\n",
        "    batch_size=512,\n",
        "    learning_rate=1e-4,\n",
        "    num_workers=4,\n",
        "    checkpoint_path=\"Basic-NLP/checkpoint\",\n",
        ")\n",
        "\n",
        "if train_bert:\n",
        "    Trainer(\n",
        "        bert,\n",
        "        train_dataset,\n",
        "        train_config,\n",
        "    ).run()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run 1 epoch...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1 / MLM: 7.5555 / NSP: 0.8509 / total loss: 8.4063\n",
            "run 2 epoch...\n",
            "Epoch:  2 / MLM: 7.5391 / NSP: 0.8540 / total loss: 8.3931\n",
            "run 3 epoch...\n",
            "Epoch:  3 / MLM: 7.5376 / NSP: 0.8411 / total loss: 8.3788\n",
            "run 4 epoch...\n",
            "Epoch:  4 / MLM: 7.5683 / NSP: 0.8456 / total loss: 8.4139\n",
            "run 5 epoch...\n",
            "Epoch:  5 / MLM: 7.5295 / NSP: 0.8430 / total loss: 8.3725\n",
            "run 6 epoch...\n",
            "Epoch:  6 / MLM: 7.5309 / NSP: 0.8278 / total loss: 8.3588\n",
            "run 7 epoch...\n",
            "Epoch:  7 / MLM: 7.5513 / NSP: 0.8135 / total loss: 8.3649\n",
            "run 8 epoch...\n",
            "Epoch:  8 / MLM: 7.5479 / NSP: 0.8109 / total loss: 8.3588\n",
            "run 9 epoch...\n",
            "Epoch:  9 / MLM: 7.5425 / NSP: 0.7893 / total loss: 8.3318\n",
            "run 10 epoch...\n",
            "Epoch: 10 / MLM: 7.5314 / NSP: 0.7688 / total loss: 8.3001\n",
            "save checkpoint...\n"
          ]
        }
      ]
    }
  ]
}