{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_04_Transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOde0nZjInm3ZaQ3Wkf6qSt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alxiom/Basic-NLP/blob/main/NLP_04_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsQ0PtHRMqff"
      },
      "source": [
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch import nn\n",
        "from torch.nn import functional as ftn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjOF3K_6MwSP",
        "outputId": "98690273-f22d-4fc5-eef4-feef4a21129e"
      },
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fdb61212b90>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1GbHTGMMw2K"
      },
      "source": [
        "device = \"cpu\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnIHn8zxMzfI"
      },
      "source": [
        "@dataclass\n",
        "class TransformerConfig:\n",
        "    seq_len: int = 16\n",
        "    vocab_size: int = 8000\n",
        "    num_encoder_layers: int = 6\n",
        "    num_decoder_layers: int = 6\n",
        "    embedding_dim: int = 512\n",
        "    num_heads: int = 6\n",
        "    hidden_dim: int = 2048\n",
        "    dropout: float = 0.1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozZ6CvUlM-d5"
      },
      "source": [
        "def positional_encoding(seq_len: int, embedding_dim: int) -> Tensor:\n",
        "    pe = np.zeros([seq_len, embedding_dim])\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(0, embedding_dim, 2):\n",
        "            pe[pos, i] = np.sin(pos / (1e+4 ** ((2 * i) / embedding_dim)))\n",
        "            pe[pos, i + 1] = np.cos(pos / (1e+4 ** ((2 * (i + 1)) / embedding_dim)))\n",
        "    return torch.from_numpy(pe).float()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn8iGLs1NFaC"
      },
      "source": [
        "def mask(x: Tensor, mask_value: float = 0.0, mask_diagonal: bool = False) -> Tensor:\n",
        "    seq_len = x.size(1)\n",
        "    indices = torch.triu_indices(seq_len, seq_len, offset=0 if mask_diagonal else 1)\n",
        "    x[:, indices[0], indices[1]] = mask_value\n",
        "    return x\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(pad_mask: Tensor, query: Tensor, key: Tensor, value: Tensor, masking: bool) -> Tensor:\n",
        "    dot_prod = query.bmm(key.transpose(1, 2))\n",
        "    if masking:\n",
        "        dot_prod = mask(dot_prod, float(\"-inf\"))\n",
        "    scale = query.size(-1) ** 0.5\n",
        "    pad_mask = pad_mask.unsqueeze(1).repeat(1, pad_mask.size(1), 1)\n",
        "    scaled_dot_product = (dot_prod / scale).masked_fill_(pad_mask, -1e+9)\n",
        "    attention = ftn.softmax(scaled_dot_product, dim=-1).bmm(value)\n",
        "    return attention"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjpCMO7nNAFY"
      },
      "source": [
        "class AttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, query_dim: int, value_dim: int, masking: bool):\n",
        "        super(AttentionHead, self).__init__()\n",
        "        self.q = nn.Linear(embedding_dim, query_dim)\n",
        "        self.k = nn.Linear(embedding_dim, query_dim)  # key_dim = query_dim\n",
        "        self.v = nn.Linear(embedding_dim, value_dim)\n",
        "        self.masking = masking\n",
        "\n",
        "    def forward(self, pad_mask: Tensor, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "        return scaled_dot_product_attention(pad_mask, self.q(query), self.k(key), self.v(value), self.masking)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads: int, embedding_dim: int, query_dim: int, value_dim: int, masking: bool = False):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [AttentionHead(embedding_dim, query_dim, value_dim, masking) for _ in range(num_heads)]\n",
        "        )\n",
        "        self.linear = nn.Linear(num_heads * value_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, pad_mask: Tensor, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "        concat_heads = torch.cat([head(pad_mask, query, key, value) for head in self.heads], dim=-1)\n",
        "        return self.linear(concat_heads)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maqpSSyMNlFG"
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim: int = 512, hidden_dim: int = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x) -> Tensor:\n",
        "        return self.ff(x)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp1pBxBwNnox"
      },
      "source": [
        "class Residual(nn.Module):\n",
        "\n",
        "    def __init__(self, sublayer: nn.Module, input_dim: int = 512, dropout: float = 0.1):\n",
        "        super(Residual, self).__init__()\n",
        "        self.sublayer = sublayer\n",
        "        self.norm = nn.LayerNorm(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, *x: Tensor) -> Tensor:\n",
        "        skip = 0 if len(x) == 1 else 1\n",
        "        return self.norm(x[skip] + self.dropout(self.sublayer(*x)))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC5TiEdbNp2x"
      },
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dim: int = 512,\n",
        "            num_heads: int = 6,\n",
        "            hidden_dim: int = 2048,\n",
        "            dropout: float = 0.1,\n",
        "    ):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        query_dim = value_dim = embedding_dim // num_heads\n",
        "        self.self_attention = Residual(\n",
        "            MultiHeadAttention(num_heads, embedding_dim, query_dim, value_dim),\n",
        "            input_dim=embedding_dim,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.feed_forward = Residual(\n",
        "            FeedForward(embedding_dim, hidden_dim),\n",
        "            input_dim=embedding_dim,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor, pad_mask: Tensor) -> Tensor:\n",
        "        x = self.self_attention(pad_mask, x, x, x)\n",
        "        x = self.feed_forward(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_layers: int = 6,\n",
        "            embedding_dim: int = 512,\n",
        "            num_heads: int = 8,\n",
        "            hidden_dim: int = 2048,\n",
        "            dropout: float = 0.1,\n",
        "    ):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = nn.ModuleList(\n",
        "            [TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor, pad_mask: Tensor) -> Tensor:\n",
        "        seq_len = x.size(1)\n",
        "        embedding_dim = x.size(2)\n",
        "        x += positional_encoding(seq_len, embedding_dim)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, pad_mask)\n",
        "        return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEUfoX9VNy4U"
      },
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dim: int = 512,\n",
        "            num_heads: int = 6,\n",
        "            hidden_dim: int = 2048,\n",
        "            dropout: float = 0.1,\n",
        "    ):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        query_dim = value_dim = embedding_dim // num_heads\n",
        "        self.masked_attention = Residual(\n",
        "            MultiHeadAttention(num_heads, embedding_dim, query_dim, value_dim, masking=True),\n",
        "            input_dim=embedding_dim,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.self_attention = Residual(\n",
        "            MultiHeadAttention(num_heads, embedding_dim, query_dim, value_dim, masking=True),\n",
        "            input_dim=embedding_dim,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.feed_forward = Residual(\n",
        "            FeedForward(embedding_dim, hidden_dim),\n",
        "            input_dim=embedding_dim,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor, context: Tensor, dec_pad_mask: Tensor, enc_pad_mask: Tensor) -> Tensor:\n",
        "        x = self.masked_attention(dec_pad_mask, x, x, x)\n",
        "        x = self.self_attention(enc_pad_mask, x, context, context)\n",
        "        x = self.feed_forward(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_layers: int = 6,\n",
        "            embedding_dim: int = 512,\n",
        "            num_heads: int = 8,\n",
        "            hidden_dim: int = 2048,\n",
        "            dropout: float = 0.1,\n",
        "    ):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = nn.ModuleList(\n",
        "            [TransformerDecoderLayer(embedding_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor, context: Tensor, dec_pad_mask: Tensor, enc_pad_mask: Tensor) -> Tensor:\n",
        "        seq_len, embedding_dim = x.size(1), x.size(2)\n",
        "        x += positional_encoding(seq_len, embedding_dim)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, context, enc_pad_mask, dec_pad_mask)\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxdKFc1cN7lh"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
        "        self.encoder = TransformerEncoder(\n",
        "            num_layers=config.num_encoder_layers,\n",
        "            embedding_dim=config.embedding_dim,\n",
        "            num_heads=config.num_heads,\n",
        "            hidden_dim=config.hidden_dim,\n",
        "            dropout=config.dropout,\n",
        "        )\n",
        "        self.decoder = TransformerDecoder(\n",
        "            num_layers=config.num_decoder_layers,\n",
        "            embedding_dim=config.embedding_dim,\n",
        "            num_heads=config.num_heads,\n",
        "            hidden_dim=config.hidden_dim,\n",
        "            dropout=config.dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, source: Tensor, target: Tensor) -> Tensor:\n",
        "        # source 임베딩 --> encoder 입력\n",
        "        # target 임베딩, source 임베딩 --> decoder 입력\n",
        "        # decoder 출력을 one-hot 변환 --> softmax\n",
        "        source_pad_mask = source == 0\n",
        "        target_pad_mask = target == 0\n",
        "        source = self.embedding(source)\n",
        "        source = self.encoder(source, source_pad_mask)\n",
        "        target = self.embedding(target)\n",
        "        target = self.decoder(target, source, target_pad_mask, source_pad_mask)\n",
        "        target = torch.matmul(target, self.embedding.weight.transpose(0, 1))\n",
        "        target = torch.softmax(target, dim=-1)\n",
        "        return target"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwuXPc69N9vY",
        "outputId": "83f686c1-1d0a-44df-9917-1a5d0d17d57c"
      },
      "source": [
        "# model config\n",
        "tfm_config = TransformerConfig()\n",
        "\n",
        "# train config\n",
        "batch_size = 64\n",
        "\n",
        "# test run\n",
        "pad = torch.zeros([batch_size, 2]).long()\n",
        "src = torch.randint(0, tfm_config.vocab_size, [batch_size, tfm_config.seq_len - 2])\n",
        "tgt = torch.randint(0, tfm_config.vocab_size, [batch_size, tfm_config.seq_len - 2])\n",
        "out = Transformer(tfm_config)(torch.cat([src, pad], dim=1), torch.cat([tgt, pad], dim=1))\n",
        "print(\"source:\", src.shape)\n",
        "print(\"target:\", tgt.shape)\n",
        "print(\"output:\", out.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source: torch.Size([64, 14])\n",
            "target: torch.Size([64, 14])\n",
            "output: torch.Size([64, 16, 8000])\n"
          ]
        }
      ]
    }
  ]
}