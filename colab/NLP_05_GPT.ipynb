{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_05_GPT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMw+WmmZfUYBWxisMt1nDFo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alxiom/Basic-NLP/blob/main/NLP_05_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebb3EWB7cR_z",
        "outputId": "3a38d405-e0b3-40d5-806c-c583b1db68e0"
      },
      "source": [
        "!mkdir checkpoint"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘checkpoint’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSkbSq6wYIQe"
      },
      "source": [
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as ftn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tvlv0ZAY6fS"
      },
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "train_gpt = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjpOu8H8Zcv5"
      },
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    seq_len: int = 16\n",
        "    vocab_size: int = 8000\n",
        "    embedding_drop_prob: float = 0.1\n",
        "    residual_drop_prob: float = 0.1\n",
        "    num_decoder_layers: int = 6\n",
        "    num_heads: int = 8\n",
        "    embedding_dim: int = 512\n",
        "    hidden_dim: int = 2048"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-_5YTAaZdYf"
      },
      "source": [
        "@dataclass\n",
        "class TrainConfig:\n",
        "    epochs: int = 10\n",
        "    batch_size: int = 64\n",
        "    learning_rate: float = 3e-4\n",
        "    grad_norm_clip: float = 1.0\n",
        "    checkpoint_path: str = None\n",
        "    num_workers: int = 0  # for DataLoader"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjzxjU2sZezi"
      },
      "source": [
        "def mask(x: Tensor, mask_value: float = 0.0):\n",
        "    seq_len = x.size(1)\n",
        "    indices = torch.triu_indices(seq_len, seq_len, offset=1)\n",
        "    x[:, indices[0], indices[1]] = mask_value\n",
        "    return x\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor, masking: bool) -> Tensor:\n",
        "    dot_prod = query.bmm(key.transpose(1, 2))\n",
        "    if masking:\n",
        "        dot_prod = mask(dot_prod, float(\"-inf\"))\n",
        "    scale = query.size(-1) ** 0.5\n",
        "    attention = ftn.softmax(dot_prod / scale, dim=-1).bmm(value)\n",
        "    return attention"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdC035ZLZhcj"
      },
      "source": [
        "class AttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, query_dim: int, value_dim: int, masking: bool):\n",
        "        super(AttentionHead, self).__init__()\n",
        "        self.q = nn.Linear(embedding_dim, query_dim)\n",
        "        self.k = nn.Linear(embedding_dim, query_dim)  # key_dim = query_dim\n",
        "        self.v = nn.Linear(embedding_dim, value_dim)\n",
        "        self.masking = masking\n",
        "\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value), self.masking)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads: int, embedding_dim: int, query_dim: int, value_dim: int, masking: bool = False):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [AttentionHead(embedding_dim, query_dim, value_dim, masking) for _ in range(num_heads)]\n",
        "        )\n",
        "        self.linear = nn.Linear(num_heads * value_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "        concat_heads = torch.cat([head(query, key, value) for head in self.heads], dim=-1)\n",
        "        return self.linear(concat_heads)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn25m_l-ZjgS"
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim: int = 512, hidden_dim: int = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ff(x)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-hth8P9ZlXJ"
      },
      "source": [
        "class Residual(nn.Module):\n",
        "\n",
        "    def __init__(self, sublayer: nn.Module, input_dim: int = 512, dropout: float = 0.1):\n",
        "        super(Residual, self).__init__()\n",
        "        self.sublayer = sublayer\n",
        "        self.norm = nn.LayerNorm(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, *x: Tensor) -> Tensor:\n",
        "        skip = 0 if len(x) == 1 else 1\n",
        "        return self.norm(x[skip] + self.dropout(self.sublayer(*x)))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4K-KanEZqV7"
      },
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        embedding_dim = config.embedding_dim\n",
        "        num_heads = config.num_heads\n",
        "        hidden_dim = config.hidden_dim\n",
        "        residual_dropout = config.residual_drop_prob\n",
        "        query_dim = value_dim = embedding_dim // num_heads\n",
        "        self.masked_attention = Residual(\n",
        "            MultiHeadAttention(num_heads, embedding_dim, query_dim, value_dim, masking=True),\n",
        "            input_dim=embedding_dim,\n",
        "            dropout=residual_dropout,\n",
        "        )\n",
        "\n",
        "        self.feed_forward = Residual(\n",
        "            FeedForward(embedding_dim, hidden_dim),\n",
        "            input_dim=embedding_dim,\n",
        "            dropout=residual_dropout,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.masked_attention(x, x, x)\n",
        "        x = self.feed_forward(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        num_layers = config.num_decoder_layers\n",
        "        self.layers = nn.ModuleList(\n",
        "            [TransformerDecoderLayer(config) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCWtvNHvZq0y"
      },
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(GPT, self).__init__()\n",
        "        self.seq_len = config.seq_len\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
        "        self.position_embedding = nn.Parameter(torch.zeros(1, config.seq_len, config.embedding_dim), requires_grad=True)\n",
        "        self.embedding_dropout = nn.Dropout(config.embedding_drop_prob)\n",
        "        self.decoder = TransformerDecoder(config)\n",
        "        self.linear = nn.Linear(config.embedding_dim, config.vocab_size, bias=False)\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weights(module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, x: Tensor, y: Tensor = None) -> (Tensor, Tensor):\n",
        "        # embedding + position embedding\n",
        "        # embedding dropout 적용\n",
        "        # transformer decoder 입력\n",
        "        # one-hot 벡터로 변환\n",
        "        # cross entropy loss 계산\n",
        "        x = self.embedding(x) + self.position_embedding[:, :x.size(1), :]\n",
        "        x = self.embedding_dropout(x)\n",
        "        x = self.decoder(x)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        loss = None\n",
        "        if y is not None:\n",
        "            loss = ftn.cross_entropy(x.view(-1, x.size(-1)), y.view(-1), ignore_index=-1)\n",
        "        return x, loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_W5T0NgZwcK"
      },
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, model, train_data, valid_data, config):\n",
        "        super(Trainer, self).__init__()\n",
        "        self.model = model\n",
        "        self.train_data = train_data\n",
        "        self.valid_data = valid_data\n",
        "        self.n_digit = train_data.n_digit\n",
        "        self.seq_len = train_data.seq_len\n",
        "        self.config = config\n",
        "        self.device = \"cpu\"\n",
        "        self.global_step = 0\n",
        "        self.start_epoch = 1\n",
        "        self.epochs = config.epochs\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        early_stop = float(\"inf\")\n",
        "\n",
        "        print(\"load valid set...\")\n",
        "        valid_data_loader = DataLoader(self.valid_data, batch_size=self.config.batch_size, shuffle=False)\n",
        "\n",
        "        print(\"run 0 epoch...\")\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            valid_loss = self.run_epoch(valid_data_loader, \"valid\")\n",
        "            print(f\"Epoch: 0 / valid loss: {valid_loss:.4f}\")\n",
        "\n",
        "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
        "            train_data_loader = DataLoader(\n",
        "                self.train_data,\n",
        "                batch_size=self.config.batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=self.config.num_workers,\n",
        "            )\n",
        "\n",
        "            print(f\"run {epoch} epoch...\")\n",
        "            self.model.train()\n",
        "            train_loss = self.run_epoch(train_data_loader, \"train\")\n",
        "            print(f\"Epoch: {epoch:2d} / train loss: {train_loss:.4f}\")\n",
        "\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                valid_loss = self.run_epoch(valid_data_loader, \"valid\")\n",
        "                print(f\"Epoch: {epoch:2d} / valid loss: {valid_loss:.4f}\")\n",
        "\n",
        "            if valid_loss < early_stop:\n",
        "                early_stop = valid_loss\n",
        "                self.save_checkpoint()\n",
        "\n",
        "    def run_epoch(self, data_loader, mode):\n",
        "        epoch_loss = 0.0\n",
        "        epoch_count = 0\n",
        "\n",
        "        for x, y in data_loader:\n",
        "            batch_size = y.size(0)\n",
        "            y_hat, batch_loss = self.model(x, y)\n",
        "\n",
        "            if mode == \"train\":\n",
        "                self.optimizer.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            epoch_loss = (epoch_loss * epoch_count + batch_loss.item() * batch_size) / (epoch_count + batch_size)\n",
        "            epoch_count += batch_size\n",
        "        return epoch_loss\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        if self.config.checkpoint_path is not None:\n",
        "            print(\"save checkpoint...\")\n",
        "            torch.save(self.model.state_dict(), f\"{self.config.checkpoint_path}/gpt.pt\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvYxbxyeZtUV"
      },
      "source": [
        "# addition demo\n",
        "class AdditionDataset(Dataset):\n",
        "\n",
        "    def __init__(self, n_digit, split):\n",
        "        super(AdditionDataset, self).__init__()\n",
        "        self.split = split  # train / valid\n",
        "        self.n_digit = n_digit\n",
        "        self.vocab_size = 10  # 10 digits [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
        "        self.seq_len = n_digit + n_digit + n_digit + 1 - 1\n",
        "\n",
        "        # split all addition problems to train / valid data\n",
        "        problems = (10 ** self.n_digit) ** 2  # total number of possible combinations\n",
        "        random_state = np.random.RandomState(42)  # make deterministic\n",
        "        permute_problems = random_state.permutation(problems)\n",
        "        num_valid = min(int(problems * 0.2), 1000)  # 20% of the whole dataset, or only up to 1000\n",
        "        self.split_problems = permute_problems[:num_valid] if split == \"valid\" else permute_problems[num_valid:]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        problem = self.split_problems[index]\n",
        "        digit = 10 ** self.n_digit\n",
        "        a = problem // digit\n",
        "        b = problem % digit\n",
        "        render = f\"{a:0{self.n_digit}d}{b:0{self.n_digit}d}{a + b:0{self.n_digit + 1}d}\"  # 03+25=28 --> \"0325028\"\n",
        "        token_seq = [int(s) for s in render]  # ex) [0, 3, 2, 5, 0, 2, 8]\n",
        "        x = torch.tensor(token_seq[:-1]).long()\n",
        "        y = torch.tensor(token_seq[1:]).long()  # shift \"right\" to predict the next token\n",
        "        y[:self.n_digit * 2 - 1] = -1  # we will only train in the output locations. -1 will mask loss to zero\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.split_problems.size"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfFnN5pQZzjn",
        "outputId": "986b478f-31a1-427a-f936-407a8307abd7"
      },
      "source": [
        "# prepare dataset\n",
        "sample_digit = 2\n",
        "train_dataset = AdditionDataset(n_digit=sample_digit, split=\"train\")\n",
        "valid_dataset = AdditionDataset(n_digit=sample_digit, split=\"valid\")\n",
        "\n",
        "# train data sample\n",
        "print(train_dataset[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 8, 9, 6, 1, 4]), tensor([-1, -1, -1,  1,  4,  4]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9h-XYBnZ4kp"
      },
      "source": [
        "# model config\n",
        "model_config = GPTConfig(\n",
        "    seq_len=train_dataset.seq_len,\n",
        "    vocab_size=train_dataset.vocab_size,\n",
        "    num_decoder_layers=2,\n",
        "    num_heads=4,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=512,\n",
        ")\n",
        "\n",
        "# init model\n",
        "gpt_model = GPT(model_config)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMwbRt0QZ67u",
        "outputId": "51cbf44d-1fb5-4065-abd9-95f91560f954"
      },
      "source": [
        "# train config\n",
        "train_config = TrainConfig(\n",
        "    epochs=80,\n",
        "    batch_size=512,\n",
        "    learning_rate=6e-4,\n",
        "    num_workers=4,\n",
        "    checkpoint_path=\"checkpoint\",\n",
        ")\n",
        "\n",
        "if train_gpt:\n",
        "    Trainer(\n",
        "        gpt_model,\n",
        "        train_dataset,\n",
        "        valid_dataset,\n",
        "        train_config,\n",
        "    ).run()\n",
        "\n",
        "# load trained model\n",
        "gpt_model.load_state_dict(torch.load(\"checkpoint/gpt.pt\"))\n",
        "gpt_model.eval()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load valid set...\n",
            "run 0 epoch...\n",
            "Epoch: 0 / valid loss: 2.3213\n",
            "run 1 epoch...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1 / train loss: 2.0118\n",
            "Epoch:  1 / valid loss: 1.7643\n",
            "save checkpoint...\n",
            "run 2 epoch...\n",
            "Epoch:  2 / train loss: 1.6448\n",
            "Epoch:  2 / valid loss: 1.5377\n",
            "save checkpoint...\n",
            "run 3 epoch...\n",
            "Epoch:  3 / train loss: 1.4724\n",
            "Epoch:  3 / valid loss: 1.4007\n",
            "save checkpoint...\n",
            "run 4 epoch...\n",
            "Epoch:  4 / train loss: 1.3709\n",
            "Epoch:  4 / valid loss: 1.3077\n",
            "save checkpoint...\n",
            "run 5 epoch...\n",
            "Epoch:  5 / train loss: 1.2643\n",
            "Epoch:  5 / valid loss: 1.1715\n",
            "save checkpoint...\n",
            "run 6 epoch...\n",
            "Epoch:  6 / train loss: 1.1120\n",
            "Epoch:  6 / valid loss: 0.9672\n",
            "save checkpoint...\n",
            "run 7 epoch...\n",
            "Epoch:  7 / train loss: 0.8903\n",
            "Epoch:  7 / valid loss: 0.7137\n",
            "save checkpoint...\n",
            "run 8 epoch...\n",
            "Epoch:  8 / train loss: 0.7296\n",
            "Epoch:  8 / valid loss: 0.6139\n",
            "save checkpoint...\n",
            "run 9 epoch...\n",
            "Epoch:  9 / train loss: 0.6223\n",
            "Epoch:  9 / valid loss: 0.4854\n",
            "save checkpoint...\n",
            "run 10 epoch...\n",
            "Epoch: 10 / train loss: 0.5325\n",
            "Epoch: 10 / valid loss: 0.4091\n",
            "save checkpoint...\n",
            "run 11 epoch...\n",
            "Epoch: 11 / train loss: 0.4513\n",
            "Epoch: 11 / valid loss: 0.3375\n",
            "save checkpoint...\n",
            "run 12 epoch...\n",
            "Epoch: 12 / train loss: 0.3819\n",
            "Epoch: 12 / valid loss: 0.2938\n",
            "save checkpoint...\n",
            "run 13 epoch...\n",
            "Epoch: 13 / train loss: 0.3463\n",
            "Epoch: 13 / valid loss: 0.2476\n",
            "save checkpoint...\n",
            "run 14 epoch...\n",
            "Epoch: 14 / train loss: 0.2967\n",
            "Epoch: 14 / valid loss: 0.2455\n",
            "save checkpoint...\n",
            "run 15 epoch...\n",
            "Epoch: 15 / train loss: 0.2917\n",
            "Epoch: 15 / valid loss: 0.1904\n",
            "save checkpoint...\n",
            "run 16 epoch...\n",
            "Epoch: 16 / train loss: 0.2480\n",
            "Epoch: 16 / valid loss: 0.1648\n",
            "save checkpoint...\n",
            "run 17 epoch...\n",
            "Epoch: 17 / train loss: 0.2154\n",
            "Epoch: 17 / valid loss: 0.1355\n",
            "save checkpoint...\n",
            "run 18 epoch...\n",
            "Epoch: 18 / train loss: 0.1953\n",
            "Epoch: 18 / valid loss: 0.1226\n",
            "save checkpoint...\n",
            "run 19 epoch...\n",
            "Epoch: 19 / train loss: 0.1790\n",
            "Epoch: 19 / valid loss: 0.1134\n",
            "save checkpoint...\n",
            "run 20 epoch...\n",
            "Epoch: 20 / train loss: 0.1676\n",
            "Epoch: 20 / valid loss: 0.1137\n",
            "run 21 epoch...\n",
            "Epoch: 21 / train loss: 0.1516\n",
            "Epoch: 21 / valid loss: 0.0859\n",
            "save checkpoint...\n",
            "run 22 epoch...\n",
            "Epoch: 22 / train loss: 0.1407\n",
            "Epoch: 22 / valid loss: 0.0709\n",
            "save checkpoint...\n",
            "run 23 epoch...\n",
            "Epoch: 23 / train loss: 0.1238\n",
            "Epoch: 23 / valid loss: 0.0666\n",
            "save checkpoint...\n",
            "run 24 epoch...\n",
            "Epoch: 24 / train loss: 0.1171\n",
            "Epoch: 24 / valid loss: 0.0672\n",
            "run 25 epoch...\n",
            "Epoch: 25 / train loss: 0.1117\n",
            "Epoch: 25 / valid loss: 0.0696\n",
            "run 26 epoch...\n",
            "Epoch: 26 / train loss: 0.1131\n",
            "Epoch: 26 / valid loss: 0.0648\n",
            "save checkpoint...\n",
            "run 27 epoch...\n",
            "Epoch: 27 / train loss: 0.1153\n",
            "Epoch: 27 / valid loss: 0.0611\n",
            "save checkpoint...\n",
            "run 28 epoch...\n",
            "Epoch: 28 / train loss: 0.0932\n",
            "Epoch: 28 / valid loss: 0.0513\n",
            "save checkpoint...\n",
            "run 29 epoch...\n",
            "Epoch: 29 / train loss: 0.0892\n",
            "Epoch: 29 / valid loss: 0.0591\n",
            "run 30 epoch...\n",
            "Epoch: 30 / train loss: 0.0894\n",
            "Epoch: 30 / valid loss: 0.0442\n",
            "save checkpoint...\n",
            "run 31 epoch...\n",
            "Epoch: 31 / train loss: 0.0802\n",
            "Epoch: 31 / valid loss: 0.0524\n",
            "run 32 epoch...\n",
            "Epoch: 32 / train loss: 0.0901\n",
            "Epoch: 32 / valid loss: 0.0387\n",
            "save checkpoint...\n",
            "run 33 epoch...\n",
            "Epoch: 33 / train loss: 0.0859\n",
            "Epoch: 33 / valid loss: 0.0560\n",
            "run 34 epoch...\n",
            "Epoch: 34 / train loss: 0.0850\n",
            "Epoch: 34 / valid loss: 0.0680\n",
            "run 35 epoch...\n",
            "Epoch: 35 / train loss: 0.0821\n",
            "Epoch: 35 / valid loss: 0.0345\n",
            "save checkpoint...\n",
            "run 36 epoch...\n",
            "Epoch: 36 / train loss: 0.0778\n",
            "Epoch: 36 / valid loss: 0.0411\n",
            "run 37 epoch...\n",
            "Epoch: 37 / train loss: 0.0645\n",
            "Epoch: 37 / valid loss: 0.0439\n",
            "run 38 epoch...\n",
            "Epoch: 38 / train loss: 0.0745\n",
            "Epoch: 38 / valid loss: 0.0409\n",
            "run 39 epoch...\n",
            "Epoch: 39 / train loss: 0.0670\n",
            "Epoch: 39 / valid loss: 0.0316\n",
            "save checkpoint...\n",
            "run 40 epoch...\n",
            "Epoch: 40 / train loss: 0.0583\n",
            "Epoch: 40 / valid loss: 0.0415\n",
            "run 41 epoch...\n",
            "Epoch: 41 / train loss: 0.0695\n",
            "Epoch: 41 / valid loss: 0.0425\n",
            "run 42 epoch...\n",
            "Epoch: 42 / train loss: 0.0640\n",
            "Epoch: 42 / valid loss: 0.0351\n",
            "run 43 epoch...\n",
            "Epoch: 43 / train loss: 0.0581\n",
            "Epoch: 43 / valid loss: 0.0299\n",
            "save checkpoint...\n",
            "run 44 epoch...\n",
            "Epoch: 44 / train loss: 0.0605\n",
            "Epoch: 44 / valid loss: 0.0294\n",
            "save checkpoint...\n",
            "run 45 epoch...\n",
            "Epoch: 45 / train loss: 0.0586\n",
            "Epoch: 45 / valid loss: 0.0246\n",
            "save checkpoint...\n",
            "run 46 epoch...\n",
            "Epoch: 46 / train loss: 0.0574\n",
            "Epoch: 46 / valid loss: 0.0312\n",
            "run 47 epoch...\n",
            "Epoch: 47 / train loss: 0.0531\n",
            "Epoch: 47 / valid loss: 0.0233\n",
            "save checkpoint...\n",
            "run 48 epoch...\n",
            "Epoch: 48 / train loss: 0.0566\n",
            "Epoch: 48 / valid loss: 0.0316\n",
            "run 49 epoch...\n",
            "Epoch: 49 / train loss: 0.0599\n",
            "Epoch: 49 / valid loss: 0.0418\n",
            "run 50 epoch...\n",
            "Epoch: 50 / train loss: 0.0596\n",
            "Epoch: 50 / valid loss: 0.0201\n",
            "save checkpoint...\n",
            "run 51 epoch...\n",
            "Epoch: 51 / train loss: 0.0556\n",
            "Epoch: 51 / valid loss: 0.0464\n",
            "run 52 epoch...\n",
            "Epoch: 52 / train loss: 0.0657\n",
            "Epoch: 52 / valid loss: 0.0265\n",
            "run 53 epoch...\n",
            "Epoch: 53 / train loss: 0.0530\n",
            "Epoch: 53 / valid loss: 0.0301\n",
            "run 54 epoch...\n",
            "Epoch: 54 / train loss: 0.0484\n",
            "Epoch: 54 / valid loss: 0.0207\n",
            "run 55 epoch...\n",
            "Epoch: 55 / train loss: 0.0493\n",
            "Epoch: 55 / valid loss: 0.0303\n",
            "run 56 epoch...\n",
            "Epoch: 56 / train loss: 0.0435\n",
            "Epoch: 56 / valid loss: 0.0231\n",
            "run 57 epoch...\n",
            "Epoch: 57 / train loss: 0.0454\n",
            "Epoch: 57 / valid loss: 0.0427\n",
            "run 58 epoch...\n",
            "Epoch: 58 / train loss: 0.0474\n",
            "Epoch: 58 / valid loss: 0.0201\n",
            "save checkpoint...\n",
            "run 59 epoch...\n",
            "Epoch: 59 / train loss: 0.0429\n",
            "Epoch: 59 / valid loss: 0.0192\n",
            "save checkpoint...\n",
            "run 60 epoch...\n",
            "Epoch: 60 / train loss: 0.0419\n",
            "Epoch: 60 / valid loss: 0.0211\n",
            "run 61 epoch...\n",
            "Epoch: 61 / train loss: 0.0458\n",
            "Epoch: 61 / valid loss: 0.0186\n",
            "save checkpoint...\n",
            "run 62 epoch...\n",
            "Epoch: 62 / train loss: 0.0413\n",
            "Epoch: 62 / valid loss: 0.0220\n",
            "run 63 epoch...\n",
            "Epoch: 63 / train loss: 0.0457\n",
            "Epoch: 63 / valid loss: 0.0196\n",
            "run 64 epoch...\n",
            "Epoch: 64 / train loss: 0.0443\n",
            "Epoch: 64 / valid loss: 0.0211\n",
            "run 65 epoch...\n",
            "Epoch: 65 / train loss: 0.0384\n",
            "Epoch: 65 / valid loss: 0.0174\n",
            "save checkpoint...\n",
            "run 66 epoch...\n",
            "Epoch: 66 / train loss: 0.0328\n",
            "Epoch: 66 / valid loss: 0.0190\n",
            "run 67 epoch...\n",
            "Epoch: 67 / train loss: 0.0397\n",
            "Epoch: 67 / valid loss: 0.0171\n",
            "save checkpoint...\n",
            "run 68 epoch...\n",
            "Epoch: 68 / train loss: 0.0353\n",
            "Epoch: 68 / valid loss: 0.0129\n",
            "save checkpoint...\n",
            "run 69 epoch...\n",
            "Epoch: 69 / train loss: 0.0322\n",
            "Epoch: 69 / valid loss: 0.0163\n",
            "run 70 epoch...\n",
            "Epoch: 70 / train loss: 0.0349\n",
            "Epoch: 70 / valid loss: 0.0192\n",
            "run 71 epoch...\n",
            "Epoch: 71 / train loss: 0.0347\n",
            "Epoch: 71 / valid loss: 0.0116\n",
            "save checkpoint...\n",
            "run 72 epoch...\n",
            "Epoch: 72 / train loss: 0.0301\n",
            "Epoch: 72 / valid loss: 0.0155\n",
            "run 73 epoch...\n",
            "Epoch: 73 / train loss: 0.0384\n",
            "Epoch: 73 / valid loss: 0.0157\n",
            "run 74 epoch...\n",
            "Epoch: 74 / train loss: 0.0342\n",
            "Epoch: 74 / valid loss: 0.0129\n",
            "run 75 epoch...\n",
            "Epoch: 75 / train loss: 0.0270\n",
            "Epoch: 75 / valid loss: 0.0072\n",
            "save checkpoint...\n",
            "run 76 epoch...\n",
            "Epoch: 76 / train loss: 0.0236\n",
            "Epoch: 76 / valid loss: 0.0103\n",
            "run 77 epoch...\n",
            "Epoch: 77 / train loss: 0.0274\n",
            "Epoch: 77 / valid loss: 0.0083\n",
            "run 78 epoch...\n",
            "Epoch: 78 / train loss: 0.0254\n",
            "Epoch: 78 / valid loss: 0.0147\n",
            "run 79 epoch...\n",
            "Epoch: 79 / train loss: 0.0264\n",
            "Epoch: 79 / valid loss: 0.0085\n",
            "run 80 epoch...\n",
            "Epoch: 80 / train loss: 0.0282\n",
            "Epoch: 80 / valid loss: 0.0120\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (embedding): Embedding(10, 128)\n",
              "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
              "  (decoder): TransformerDecoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerDecoderLayer(\n",
              "        (masked_attention): Residual(\n",
              "          (sublayer): MultiHeadAttention(\n",
              "            (heads): ModuleList(\n",
              "              (0): AttentionHead(\n",
              "                (q): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (k): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (v): Linear(in_features=128, out_features=32, bias=True)\n",
              "              )\n",
              "              (1): AttentionHead(\n",
              "                (q): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (k): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (v): Linear(in_features=128, out_features=32, bias=True)\n",
              "              )\n",
              "              (2): AttentionHead(\n",
              "                (q): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (k): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (v): Linear(in_features=128, out_features=32, bias=True)\n",
              "              )\n",
              "              (3): AttentionHead(\n",
              "                (q): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (k): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (v): Linear(in_features=128, out_features=32, bias=True)\n",
              "              )\n",
              "            )\n",
              "            (linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): Residual(\n",
              "          (sublayer): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): TransformerDecoderLayer(\n",
              "        (masked_attention): Residual(\n",
              "          (sublayer): MultiHeadAttention(\n",
              "            (heads): ModuleList(\n",
              "              (0): AttentionHead(\n",
              "                (q): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (k): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (v): Linear(in_features=128, out_features=32, bias=True)\n",
              "              )\n",
              "              (1): AttentionHead(\n",
              "                (q): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (k): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (v): Linear(in_features=128, out_features=32, bias=True)\n",
              "              )\n",
              "              (2): AttentionHead(\n",
              "                (q): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (k): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (v): Linear(in_features=128, out_features=32, bias=True)\n",
              "              )\n",
              "              (3): AttentionHead(\n",
              "                (q): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (k): Linear(in_features=128, out_features=32, bias=True)\n",
              "                (v): Linear(in_features=128, out_features=32, bias=True)\n",
              "              )\n",
              "            )\n",
              "            (linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): Residual(\n",
              "          (sublayer): FeedForward(\n",
              "            (ff): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=128, out_features=10, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMPEYXdcZ_cW",
        "outputId": "1b9f7b60-bbc3-4733-a218-6be3ea3391e5"
      },
      "source": [
        "def generate(model, x, step):\n",
        "    seq_len = model.seq_len\n",
        "    for _ in range(step):\n",
        "        x_crop = x if x.size(1) <= seq_len else x[:, -seq_len:]  # crop left\n",
        "        logit, _ = model(x_crop)\n",
        "        logit = logit[:, -1, :]\n",
        "        _, prediction = torch.topk(logit, k=1, dim=-1)\n",
        "        x = torch.cat((x, prediction), dim=1)\n",
        "    return x\n",
        "\n",
        "\n",
        "def model_test(model, dataset, batch_size):\n",
        "    results = []\n",
        "    n_digit = dataset.n_digit\n",
        "    for x, _ in DataLoader(dataset, batch_size=batch_size):\n",
        "        query = x[:, :n_digit * 2]\n",
        "        generate_answer = generate(model, query, n_digit + 1)\n",
        "        answer = generate_answer[:, -(n_digit + 1):]\n",
        "        base10 = torch.tensor([10 ** i for i in range(n_digit + 1)]).long().flip(0).unsqueeze(0)\n",
        "        a = (query[:, :n_digit] * base10[:, (n_digit - 1):]).sum(1)\n",
        "        b = (query[:, n_digit:n_digit * 2] * base10[:, (n_digit - 1):]).sum(1)\n",
        "        gt = a + b\n",
        "        answer = (answer * base10).sum(1)\n",
        "        correct = answer == gt\n",
        "        for i in range(x.size(0)):\n",
        "            results.append(int(correct[i]))\n",
        "            if not correct[i]:\n",
        "                print(f\"model prediction: {a[i]} + {b[i]} = {answer[i]} (GT = {gt[i]}) --> {correct[i]}\")\n",
        "    print(f\"final score: {sum(results)} / {len(results)} = {np.mean(results) * 100:.2f} %% correct\")\n",
        "\n",
        "\n",
        "print(\"test on train set (inner test)\")\n",
        "model_test(gpt_model, train_dataset, batch_size=1024)\n",
        "print(\"--------------------------------------------\")\n",
        "print(\"test on valid set\")\n",
        "model_test(gpt_model, valid_dataset, batch_size=1024)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test on train set (inner test)\n",
            "model prediction: 74 + 5 = 89 (GT = 79) --> False\n",
            "model prediction: 19 + 81 = 90 (GT = 100) --> False\n",
            "model prediction: 79 + 21 = 90 (GT = 100) --> False\n",
            "model prediction: 62 + 48 = 100 (GT = 110) --> False\n",
            "model prediction: 63 + 6 = 79 (GT = 69) --> False\n",
            "model prediction: 63 + 67 = 120 (GT = 130) --> False\n",
            "model prediction: 52 + 17 = 79 (GT = 69) --> False\n",
            "model prediction: 62 + 58 = 110 (GT = 120) --> False\n",
            "model prediction: 44 + 66 = 100 (GT = 110) --> False\n",
            "model prediction: 64 + 36 = 90 (GT = 100) --> False\n",
            "model prediction: 56 + 94 = 140 (GT = 150) --> False\n",
            "model prediction: 68 + 62 = 120 (GT = 130) --> False\n",
            "model prediction: 54 + 56 = 100 (GT = 110) --> False\n",
            "model prediction: 11 + 89 = 90 (GT = 100) --> False\n",
            "model prediction: 89 + 11 = 90 (GT = 100) --> False\n",
            "model prediction: 64 + 5 = 79 (GT = 69) --> False\n",
            "model prediction: 92 + 97 = 199 (GT = 189) --> False\n",
            "model prediction: 54 + 46 = 90 (GT = 100) --> False\n",
            "model prediction: 33 + 67 = 90 (GT = 100) --> False\n",
            "model prediction: 39 + 61 = 90 (GT = 100) --> False\n",
            "model prediction: 32 + 68 = 90 (GT = 100) --> False\n",
            "model prediction: 63 + 47 = 100 (GT = 110) --> False\n",
            "model prediction: 54 + 96 = 140 (GT = 150) --> False\n",
            "model prediction: 73 + 6 = 89 (GT = 79) --> False\n",
            "model prediction: 46 + 64 = 100 (GT = 110) --> False\n",
            "model prediction: 29 + 71 = 90 (GT = 100) --> False\n",
            "model prediction: 69 + 31 = 90 (GT = 100) --> False\n",
            "model prediction: 63 + 37 = 90 (GT = 100) --> False\n",
            "model prediction: 61 + 39 = 90 (GT = 100) --> False\n",
            "model prediction: 62 + 68 = 120 (GT = 130) --> False\n",
            "model prediction: 36 + 34 = 60 (GT = 70) --> False\n",
            "model prediction: 71 + 29 = 90 (GT = 100) --> False\n",
            "model prediction: 43 + 57 = 90 (GT = 100) --> False\n",
            "model prediction: 67 + 63 = 120 (GT = 130) --> False\n",
            "model prediction: 52 + 48 = 90 (GT = 100) --> False\n",
            "model prediction: 53 + 47 = 90 (GT = 100) --> False\n",
            "model prediction: 97 + 92 = 199 (GT = 189) --> False\n",
            "model prediction: 4 + 75 = 89 (GT = 79) --> False\n",
            "model prediction: 21 + 79 = 90 (GT = 100) --> False\n",
            "model prediction: 75 + 4 = 89 (GT = 79) --> False\n",
            "model prediction: 76 + 3 = 89 (GT = 79) --> False\n",
            "model prediction: 22 + 78 = 90 (GT = 100) --> False\n",
            "model prediction: 66 + 44 = 100 (GT = 110) --> False\n",
            "model prediction: 64 + 46 = 100 (GT = 110) --> False\n",
            "model prediction: 59 + 41 = 90 (GT = 100) --> False\n",
            "model prediction: 90 + 9 = 109 (GT = 99) --> False\n",
            "model prediction: 62 + 38 = 90 (GT = 100) --> False\n",
            "model prediction: 5 + 74 = 89 (GT = 79) --> False\n",
            "model prediction: 34 + 36 = 60 (GT = 70) --> False\n",
            "final score: 8951 / 9000 = 99.46 %% correct\n",
            "--------------------------------------------\n",
            "test on valid set\n",
            "model prediction: 67 + 43 = 100 (GT = 110) --> False\n",
            "model prediction: 72 + 28 = 90 (GT = 100) --> False\n",
            "model prediction: 61 + 49 = 100 (GT = 110) --> False\n",
            "model prediction: 3 + 76 = 89 (GT = 79) --> False\n",
            "final score: 996 / 1000 = 99.60 %% correct\n"
          ]
        }
      ]
    }
  ]
}